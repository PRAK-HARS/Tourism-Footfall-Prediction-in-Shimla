{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43851ef7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-14T18:25:39.549152Z",
     "iopub.status.busy": "2023-10-14T18:25:39.548795Z",
     "iopub.status.idle": "2023-10-14T18:25:39.921659Z",
     "shell.execute_reply": "2023-10-14T18:25:39.920567Z"
    },
    "papermill": {
     "duration": 0.381605,
     "end_time": "2023-10-14T18:25:39.924412",
     "exception": false,
     "start_time": "2023-10-14T18:25:39.542807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9033d6",
   "metadata": {
    "papermill": {
     "duration": 0.003968,
     "end_time": "2023-10-14T18:25:39.934632",
     "exception": false,
     "start_time": "2023-10-14T18:25:39.930664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c4e92e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T18:25:39.944827Z",
     "iopub.status.busy": "2023-10-14T18:25:39.944381Z",
     "iopub.status.idle": "2023-10-14T18:25:50.981038Z",
     "shell.execute_reply": "2023-10-14T18:25:50.979623Z"
    },
    "papermill": {
     "duration": 11.043625,
     "end_time": "2023-10-14T18:25:50.982424",
     "exception": true,
     "start_time": "2023-10-14T18:25:39.938799",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/open-IIT-DATA - Sheet1 2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x_train,x_test,y_train,y_test\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#Read the csv file\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/open-IIT-DATA - Sheet1 2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m df\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#Define the model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/open-IIT-DATA - Sheet1 2.csv'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"ANN.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1mJQdCg7-44XSmubsyjzdU_CmUk2AyBo_\n",
    "\"\"\"\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "#import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Function to preprocess the dataset to array\n",
    "def toArray(data_set):\n",
    "  x_train = data_set.iloc[:108,[1,4,7,16,22,23,28,33]].copy()\n",
    "  x_test = data_set.iloc[108:,[1,4,7,16,22,23,28,33]].copy()\n",
    "  y_train = data_set.loc[:107,'No of Tourist(I'].copy() + data_set.loc[:107,'Foreign Tourist'].copy()\n",
    "  y_test = data_set.loc[107:,'No of Tourist(I'].copy() + data_set.loc[107:,'Foreign Tourist'].copy()\n",
    "  x_train = x_train.to_numpy()\n",
    "  x_test = x_test.to_numpy()\n",
    "  y_train = y_train.to_numpy()\n",
    "  y_test = y_test.to_numpy()\n",
    "  return x_train,x_test,y_train,y_test\n",
    "\n",
    "#Read the csv file\n",
    "df = pd.read_csv('/content/open-IIT-DATA - Sheet1 2.csv')\n",
    "df\n",
    "\n",
    "#Define the model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(units=8, activation = 'relu'),\n",
    "        Dense(units=4, activation = 'relu'),\n",
    "        Dense(units=1, activation = 'linear')\n",
    "\n",
    "    ], name = \"my_model\"\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.01),\n",
    ")\n",
    "\n",
    "X_train,X_test,y_train,y_test = toArray(df)\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "y_train.shape\n",
    "\n",
    "X_train\n",
    "\n",
    "y_train\n",
    "\n",
    "model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs = 500\n",
    ")\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "#Define the metric system for evaluation\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "def me(predictions, targets):\n",
    "    return (abs(predictions-targets)).mean()\n",
    "\n",
    "rmse(predictions, y_test)\n",
    "\n",
    "me(predictions,y_test)\n",
    "\n",
    "plt.plot(predictions, color='b')\n",
    "plt.plot(y_test,color='r')\n",
    "\n",
    "#Plotting the weights vs features to get the weightage of each feature in predicting the target data\n",
    "x= model.weights\n",
    "\n",
    "x = np.array(x)\n",
    "\n",
    "x.shape\n",
    "\n",
    "plt.plot(x[0])\n",
    "\n",
    "plt.plot(x[2])\n",
    "\n",
    "plt.plot(x[1])\n",
    "\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "predictions.shape\n",
    "\n",
    "df['predicted'] = np.zeros(132)\n",
    "\n",
    "for i in range(132):\n",
    "  if i<109:\n",
    "    df.loc[i,['predicted']] = None\n",
    "  else:\n",
    "    df.loc[i,['predicted']] = predictions[i-109]\n",
    "\n",
    "df\n",
    "\n",
    "df.set_index('Timestamp', inplace = True)\n",
    "\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "loc = plticker.MultipleLocator(base=30.0) # this locator puts ticks at regular intervals\n",
    "\n",
    "#Final plot\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(df['predicted'],color='orange', label='Predicted np. of tourists')\n",
    "plt.plot(df['No of Tourist(I'], label='Actual no. of tourists')\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('No. of Tourists')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c89ae9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2205c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"KNN code.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1cx02N62OwZq-Ja8RdV7-vGIobXYRzjVS\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "df = pd.read_csv('/content/drive/MyDrive/Data/Lagged.csv')\n",
    "df1 = pd.read_csv('/content/drive/MyDrive/Data/data_tourist.csv')\n",
    "\n",
    "try:\n",
    "  df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y')\n",
    "except ValueError:\n",
    "  print(\"Error: Date format mismatch in your dataset.\")\n",
    "\n",
    "df['Numerical_Date'] = df.index + 1\n",
    "df1['Numerical_Date'] = df1.index + 1\n",
    "x=df['Numerical_Date'].values\n",
    "y=df['SUM'].values\n",
    "x1= df1['Numerical_Date'].values\n",
    "y1 = df1['No of Tourist'].values\n",
    "df.head()\n",
    "\n",
    "#Implementing KNN\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to calculate the Euclidean distance between two points.\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "class KNNRegressor:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    # Method to fit the KNN model with training data.\n",
    "    def fit(self, X, y):\n",
    "        # Store the training data and labels.\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    # Method to make predictions for a set of input data points.\n",
    "    def predict(self, X):\n",
    "        # For each data point in X, make a prediction and return an array of predicted values.\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    # Internal method to predict a single data point 'x'.\n",
    "    def _predict(self, x):\n",
    "        # Calculate the Euclidean distance between the input point 'x' and all training data points.\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "\n",
    "        # Get the indices of the 'k' nearest data points from the training data.\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # Get the labels (target values) of the 'k' nearest neighbors.\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        # Return the mean (average) of the labels of the 'k' nearest neighbors as the predicted value for 'x'.\n",
    "        return np.mean(k_nearest_labels)\n",
    "\n",
    "# Split the dataset into training and testing sets for the first set of features and labels.\n",
    "# The 'random_state' parameter ensures reproducibility of the split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Adjust the scale of 'y_test' by dividing it by 10,000. This may be a data-specific transformation.\n",
    "y_test = y_test / 10000\n",
    "\n",
    "# Create an instance of the KNNRegressor class with 'k' set to 3 (number of neighbors to consider).\n",
    "knn_regressor = KNNRegressor(k=3)\n",
    "\n",
    "# Fit the KNN model with the training data (X_train and y_train).\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Adjust the scale of 'y_pred' by dividing it by 10,000.\n",
    "y_pred = knn_regressor.predict(X_test)\n",
    "y_pred = y_pred / 10000\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE) as the mean absolute difference between 'y_test' and 'y_pred'.\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "\n",
    "# Calculate the Root Mean Square Error (RMSE) as the square root of the mean squared difference between 'y_test' and 'y_pred'.\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Root Mean Square Error (RMSE): {rmse}')\n",
    "\n",
    "predicted_values = y_pred\n",
    "y_test1 = y_test1/10000\n",
    "Actual = y_test1\n",
    "\n",
    "#Plotting the results\n",
    "plt.figure(figsize= (10, 6))\n",
    "plt.title(\" Tourist and search index with 1 lag\")\n",
    "plt.plot( xt , Actual , label= 'No. of tourist'  )\n",
    "\n",
    "plt.plot(xt , predicted_values , label= 'Search index + tourist ' )\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89de2d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**SARIMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc863d6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"TouristPrediction.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1olUmGxJaBwHrlKBu-6PCwAjAlmSR793a\n",
    "\"\"\"\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('/content/open-IIT-DATA - Sheet1 (1).csv', index_col='Timestamp')\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(df[['weather shimla: (Worldwide)','Adventure activities: (Worldwide)','Shimla to manali: (Worldwide)','Shimla Airport']])\n",
    "\n",
    "scaled_data=scaler.transform(df[['weather shimla: (Worldwide)','Adventure activities: (Worldwide)','Shimla to manali: (Worldwide)','Shimla Airport']])\n",
    "\n",
    "pca=PCA(n_components=1)\n",
    "\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "x_pca=pca.transform(scaled_data)\n",
    "\n",
    "df['pca']= x_pca\n",
    "\n",
    "def calc_rmse_mae(actual_data,predicted_data):\n",
    "  mae = mean_absolute_error(actual_data, predicted_data)\n",
    "\n",
    "  rmse = np.sqrt(mean_squared_error(actual_data, predicted_data))\n",
    "\n",
    "  print(f\"MAE: {mae}\")\n",
    "  print(f\"RMSE: {rmse}\")\n",
    "\n",
    "df['No of Tourist'] = df['No of Tourist']/10000\n",
    "df['Foreign Tourist'] = df['Foreign Tourist']/1000\n",
    "\n",
    "df['pca_sum']=df['Hotels in shimla']+df['Cheap hotels ']+df['budget hotels ']+df['Travel guide']+df['Shimla flights: (Worldwide)']+df['train shimla: (Worldwide)']+df['Shopping Shimla']+df['Shimla maps: (Worldwide)']+df['pca']+df['No of Tourist']\n",
    "\n",
    "df['Sum'] = df['Sum']/12.5\n",
    "df['pca_sum'] = df['pca_sum']/12.5\n",
    "\n",
    "df['Hotels in shimla Sum'] = (df['No of Tourist']+df['Hotels in shimla'])/2\n",
    "df['Cheap hotels Sum'] = (df['No of Tourist']+df['Cheap hotels '])/2.5\n",
    "df['budget hotels Sum'] = (df['No of Tourist']+df['budget hotels '])/1.4\n",
    "df['Travel guide Sum'] = (df['No of Tourist']+df['Travel guide'])/2\n",
    "df['Shimla flights: (Worldwide) Sum'] = (df['No of Tourist']+df['Shimla flights: (Worldwide)'])/3\n",
    "df['train shimla: (Worldwide) Sum'] = (df['No of Tourist']+df['train shimla: (Worldwide)'])/3\n",
    "df['Shopping Shimla Sum'] = (df['No of Tourist']+df['Shopping Shimla'])/3\n",
    "df['Shimla maps: (Worldwide) Sum'] = (df['No of Tourist']+df['Shimla maps: (Worldwide)'])/2\n",
    "\n",
    "df_train = df[0:100].copy()\n",
    "df_test = df[100:132].copy()\n",
    "\n",
    "adf, pval, usedlag, nobs, crit_vals, icbest =  adfuller(df_train['No of Tourist'])\n",
    "print('ADF test statistic:', adf)\n",
    "print('ADF p-values:', pval)\n",
    "print('ADF number of lags used:', usedlag)\n",
    "print('ADF number of observations:', nobs)\n",
    "print('ADF critical values:', crit_vals)\n",
    "print('ADF best information criterion:', icbest)\n",
    "\n",
    "df_train['Seasonal Difference']=df_train['No of Tourist']-df_train['No of Tourist'].shift(8)\n",
    "\n",
    "adf, pval, usedlag, nobs, crit_vals, icbest =  adfuller(df_train['Seasonal Difference'].dropna())\n",
    "print('ADF test statistic:', adf)\n",
    "print('ADF p-values:', pval)\n",
    "print('ADF number of lags used:', usedlag)\n",
    "print('ADF number of observations:', nobs)\n",
    "print('ADF critical values:', crit_vals)\n",
    "print('ADF best information criterion:', icbest)\n",
    "\n",
    "df_train['Seasonal Difference'].plot(figsize=(12,8))\n",
    "\n",
    "plot_pacf(df_train['Seasonal Difference'].dropna());\n",
    "plot_acf(df_train['Seasonal Difference'].dropna());\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['No of Tourist'].dropna(),order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['No of Tourist'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('number of tourists ')\n",
    "plt.ylabel('Normalized number of tourists')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['No of Tourist'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['Hotels in shimla Sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['No of Tourist'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('Hotels in shimla + No of tourists')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['Hotels in shimla Sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['budget hotels Sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['No of Tourist'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('budget hotels + No of tourists')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['budget hotels Sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['Cheap hotels Sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['No of Tourist'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('cheap hotels + No of tourists')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['Cheap hotels Sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['Travel guide Sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['No of Tourist'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('Travel guide + No of tourists')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['Travel guide Sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['Shimla flights: (Worldwide) Sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['No of Tourist'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('Shimla flights: (Worldwide) + No of tourists')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['Shimla flights: (Worldwide) Sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['train shimla: (Worldwide) Sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['No of Tourist'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('train shimla: (Worldwide) + No of tourists')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['train shimla: (Worldwide) Sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['Shopping Shimla Sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['No of Tourist'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('Shopping Shimla + No of tourists')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['Shopping Shimla Sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['Shimla maps: (Worldwide) Sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['No of Tourist'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('Shimla maps: (Worldwide) + No of tourists')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['Shimla maps: (Worldwide) Sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['Sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['Sum'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('Search Index')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['Sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))\n",
    "\n",
    "model_SARIMA=sm.tsa.statespace.SARIMAX(df_train['pca_sum'],order=(2, 1, 3),seasonal_order=(2,1,3,12))\n",
    "results_SARIMA=model_SARIMA.fit()\n",
    "plt.figure(figsize=(12,8))\n",
    "df['pca_sum'].plot()\n",
    "plt.plot(results_SARIMA.predict(start=100,end=131,dynamic=True) , label='predicted')\n",
    "plt.title('Search Index with PCA')\n",
    "plt.ylabel('Normalized volume')\n",
    "plt.legend()\n",
    "\n",
    "calc_rmse_mae(df['pca_sum'][100:132],results_SARIMA.predict(start=100,end=131,dynamic=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c691a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**SARIMAX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92cc35",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# #                                           Sarimax Model\n",
    "\n",
    "# ####  Importing the required libraries \n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf , plot_pacf\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_SEED = np.random.seed(0)\n",
    "\n",
    "\n",
    "# ####   Importing the  dataset  as shimla\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# dataset below is the data of summation of all the best features including their lagged value \n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "shimla = pd.read_csv(\"D:\\DATA ANALYYSIS\\data analysis\\.ipynb_checkpoints\\datasets\\open_new_iit.csv\", index_col = 'Timestamp')\n",
    "dataset = shimla.iloc[:,[29]]\n",
    "dataset= dataset/10\n",
    "shimla.head()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# dataset2 is data of Number of Tourist \n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "dataset2 = shimla.iloc[:,[26]]\n",
    "dataset2 = dataset2/1000\n",
    "print(dataset2)\n",
    "\n",
    "\n",
    "# #### splitting the dataset into test and training set (20:80)\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "train = dataset.iloc[:105,:]\n",
    "test = dataset.iloc[106:,:]\n",
    "train_exp=dataset.iloc[:50,:]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train_exp.dropna(),label ='actual')\n",
    "plt.title('HOTELS IN SHIMLA ',fontsize=20)\n",
    "plt.ylabel('SEARCHES',fontsize=16)\n",
    "\n",
    "\n",
    "# ####  using the seasonal decompose module from stats model library to find seasonilty trend and residue \n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "plt.rc('figure',figsize=(12,8))\n",
    "plt.rc('font',size=15)\n",
    "\n",
    "result = seasonal_decompose(train_exp['sum features.1'].dropna(),model='additive', period= 7)\n",
    "fig = result.plot()\n",
    "\n",
    "\n",
    "# ####  plotting acf and pacf plots to find parameters of sarimax\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "plot_acf(train.dropna())\n",
    "plot_pacf(train.dropna())\n",
    "\n",
    "\n",
    "# ####  training the sarimax model \n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "model= sm.tsa.statespace.SARIMAX(train.dropna(),order=(0,0,0),seasonal_order=(1,1,1,12))\n",
    "results=model.fit()\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "import matplotlib.ticker as plticker\n",
    "loc = plticker.MultipleLocator(base=30)\n",
    "fig,ax=plt.subplots()\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "plt.plot(dataset,label='Search Index + Tourist')\n",
    "plt.plot(results.predict(start=104,end=131,dynamic=True) , label='Predicted')\n",
    "plt.title('Tourist and Search Index with 0 and 1 lag ')\n",
    "plt.ylabel('Normalised Volume')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "\n",
    "# #### rmse and mae to evaluate the model \n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    " \n",
    "sqrt(mean_squared_error(test.dropna(), results.predict(start=106,end=131,dynamic=True))) \n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(test.dropna(), results.predict(start=106,end=131,dynamic=True))\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "np.mean(dataset)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11488482",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**EXPONENTIAL SMOOTHING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9895e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Arima_Model1.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1aiX0gtBqCrx0iRoceTz3yw5X7HNy1tNP\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the first time series data\n",
    "df = pd.read_csv('/content/data_tourist.csv', index_col='Timestamp')\n",
    "df = df.dropna()\n",
    "print(\"Shape of Data\", df.shape)\n",
    "\n",
    "# Plot the original time series data\n",
    "df.plot(figsize=(12, 5))\n",
    "\n",
    "# Perform Augmented Dickey-Fuller test for stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "adf, pval, usedlag, nobs, crit_vals, icbest = adfuller(df.No_of_Tourist.values)\n",
    "print('ADF test statistic:', adf)\n",
    "print('ADF p-values:', pval)\n",
    "print('ADF number of lags used:', usedlag)\n",
    "print('ADF number of observations:', nobs)\n",
    "print('ADF critical values:', crit_vals)\n",
    "print('ADF best information criterion:', icbest)\n",
    "\n",
    "# Differencing to make the time series stationary\n",
    "prev_No_of_Tourist = df.No_of_Tourist.shift()\n",
    "df = df.No_of_Tourist - prev_No_of_Tourist\n",
    "df.plot(figsize=(12, 5))\n",
    "\n",
    "# Check for missing values\n",
    "df.isna()\n",
    "\n",
    "# Prepare the training and test data\n",
    "train = df['No_of_Tourist'].iloc[:-30] / 10000.0\n",
    "test = df['No_of_Tourist'].iloc[-30:] / 10000.0\n",
    "\n",
    "# Create an Exponential Smoothing model\n",
    "model = sm.tsa.ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=12)\n",
    "model = model.fit()\n",
    "model.summary()\n",
    "\n",
    "# Make predictions\n",
    "start = len(train)\n",
    "end = len(train) + len(test) - 1\n",
    "pred = model.predict(start=start, end=end).rename('Predictions')\n",
    "pred.index = df.index[start:end + 1]\n",
    "\n",
    "# Plot the test data and predictions\n",
    "test.plot(legend=True, figsize=(8, 5))\n",
    "pred.plot(legend=True)\n",
    "\n",
    "# Calculate MAE and RMSE\n",
    "rmse = np.sqrt(mean_squared_error(test, pred))\n",
    "print(\"MAE:\", mean_absolute_error(test, pred))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Load the second time series data\n",
    "df = pd.read_csv('/content/open_new_iit.csv', index_col='Timestamp')\n",
    "df = df.dropna()\n",
    "print(\"Shape of Data\", df.shape)\n",
    "\n",
    "# Prepare the training and test data\n",
    "train = df['sum(lag)'].iloc[:-30] / 10.0\n",
    "test = df['sum(lag)'].iloc[-30:] / 10.0\n",
    "\n",
    "# Create an Exponential Smoothing model\n",
    "model = sm.tsa.ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=12)\n",
    "model = model fit()\n",
    "model.summary()\n",
    "\n",
    "# Make predictions\n",
    "start = len(train)\n",
    "end = len(train) + len(test) - 1\n",
    "pred = model.predict(start=start, end=end).rename('Predictions')\n",
    "pred.index = df.index[start:end + 1]\n",
    "\n",
    "# Plot the test data and predictions\n",
    "ax = test.plot(legend=True, figsize=(8, 5), label='Search Trend (with lag)')\n",
    "ax.set_ylabel(\"Normalized Volume\")\n",
    "pred.plot(legend=True)\n",
    "\n",
    "# Calculate MAE and RMSE\n",
    "rmse = np.sqrt(mean_squared_error(test, pred))\n",
    "print(\"MAE:\", mean_absolute_error(test, pred))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Load the third time series data\n",
    "train = df['Sum'].iloc[:-30] / 10.0\n",
    "test = df['Sum'].iloc[-30:] / 10.0\n",
    "\n",
    "# Create an Exponential Smoothing model\n",
    "model = sm.tsa.ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=12)\n",
    "model = model.fit()\n",
    "model.summary()\n",
    "\n",
    "# Make predictions\n",
    "start = len(train)\n",
    "end = len(train) + len(test) - 1\n",
    "pred = model.predict(start=start, end=end).rename('Predictions')\n",
    "pred.index = df.index[start:end + 1]\n",
    "\n",
    "# Plot the test data and predictions\n",
    "ax = test.plot(legend=True, figsize=(8, 5), label='Search Trend')\n",
    "ax.set_ylabel(\"Normalized Volume\")\n",
    "pred.plot(legend=True)\n",
    "\n",
    "# Calculate MAE and RMSE\n",
    "rmse = np.sqrt(mean_squared_error(test, pred))\n",
    "print(\"MAE:\", mean_absolute_error(test, pred))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34862d6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**CATBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e554d2e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import catboost\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "df=pd.read_csv(\"D:\\openiitda\\SUMtotal.csv\") #extract the total sum\n",
    "ht=pd.read_csv(\"D:\\openiitda\\data_tourist 1.csv\") #extract number of tourist\n",
    "try:\n",
    "    df['Timestamp']=pd.to_datetime(df['Timestamp'],format='%d-%m-%Y') #convert timestamp to numerical date\n",
    "except ValueError:\n",
    "    print(\"Error\")\n",
    "df['Numerical_date']=df['Timestamp'].dt.month\n",
    "try:\n",
    "    ht['Timestamp']=pd.to_datetime(ht['Timestamp'],format='%d/%m/%Y')\n",
    "except ValueError:\n",
    "    print(\"Error\")\n",
    "ht['Numerical_date']=ht['Timestamp'].dt.month \n",
    "X=df.drop('SUM',axis=1)\t #segregate x and y\n",
    "y=df['SUM']\n",
    "Xt=ht.drop('No of Tourist(I',axis=1)\t\n",
    "yt=ht['No of Tourist(I']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #test train split\n",
    "model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE')\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100, verbose=100) #train the model\n",
    "Xt_train, Xt_test, yt_train, yt_test = train_test_split(Xt, yt, test_size=0.2, random_state=42)\n",
    "y_pred = model.predict(X_test) #predict using train data\n",
    "y_pred\n",
    "yt=yt_test\n",
    "mse = mean_squared_error(y_test, y_pred) #calculate mean squared error\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "from math import sqrt\n",
    "print(sqrt(mse))\n",
    "from sklearn.metrics import mean_absolute_error #calculate mean absolute error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "actual_values=yt    #plot graph between tourist and SUM\n",
    "predicted_values=y_pred\n",
    "\n",
    "\n",
    "time_index=range(len(actual_values))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Tourists and search index with lag\")\n",
    "plt.plot(time_index, actual_values, label='Number of Tourists',  marker='')\n",
    "plt.plot(time_index, predicted_values, label='Tourists+search trends(with lag)', color='orange', marker='')\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalised volumes\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660c0a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95431bfc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"RNN_LSTM.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1wShfrNd72saKh86F22nF7d4Xihr17Rx4\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load data from a CSV file\n",
    "data = pd.read_csv('/Users/vedantahazra/Downloads/open-IIT-DATA - Sheet1 2.csv')\n",
    "\n",
    "# Select relevant columns\n",
    "cols = ['Timestamp','No of Tourist(I']\n",
    "data = data[cols]\n",
    "data\n",
    "\n",
    "# Check if a GPU is available, and set the device accordingly\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "# Define a function to prepare the dataframe for LSTM\n",
    "def prepare_dataframe_for_lstm(df, n_steps):\n",
    "    df = dc(df)\n",
    "\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "    # Create lag features for time series data\n",
    "    for i in range(1, n_steps+1):\n",
    "        df[f'No of Tourist(I((t-{i})'] = df['No of Tourist(I'].shift(i)\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Set the number of lookback steps for time series\n",
    "lookback = 7\n",
    "\n",
    "# Prepare the dataframe for LSTM\n",
    "shifted_df = prepare_dataframe_for_lstm(data, lookback)\n",
    "shifted_df\n",
    "\n",
    "# Convert the dataframe to a NumPy array\n",
    "shifted_df_as_np = shifted_df.to_numpy()\n",
    "\n",
    "shifted_df_as_np\n",
    "\n",
    "shifted_df_as_np.shape\n",
    "\n",
    "# Apply Min-Max scaling to the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "shifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n",
    "\n",
    "shifted_df_as_np\n",
    "\n",
    "# Separate the features (X) and target (y)\n",
    "X = shifted_df_as_np[:, 1:]\n",
    "y = shifted_df_as_np[:, 0]\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "# Flip the features along the time axis\n",
    "X = dc(np.flip(X, axis=1))\n",
    "X\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "split_index = int(len(X) * 0.8)\n",
    "\n",
    "split_index\n",
    "\n",
    "X_train = X[:split_index]\n",
    "X_test = X[split_index:]\n",
    "\n",
    "y_train = y[:split_index]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# Reshape the data for compatibility with the LSTM model\n",
    "X_train = X_train.reshape((-1, lookback, 1))\n",
    "X_test = X_test.reshape((-1, lookback, 1))\n",
    "\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a custom dataset for PyTorch\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "train_dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for _, batch in enumerate(train_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break\n",
    "\n",
    "# Define an LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        # Create an LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # Create a fully connected (linear) layer for prediction\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the LSTM model\n",
    "model = LSTM(1, 6, 1)\n",
    "model.to(device)\n",
    "model\n",
    "\n",
    "# Define training and validation functions\n",
    "def train_one_epoch():\n",
    "    # Set the model in training mode\n",
    "    model.train(True)\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        output = model(x_batch)\n",
    "        loss = loss_function(output, y_batch)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 100 == 99:  # print every 100 batches\n",
    "            avg_loss_across_batches = running_loss / 100\n",
    "            print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1,\n",
    "                                                    avg_loss_across_batches))\n",
    "            running_loss = 0.0\n",
    "    print()\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "def validate_one_epoch():\n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_index, batch in enumerate(test_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(x_batch)\n",
    "            loss = loss_function(output, y_batch)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    avg_loss_across_batches = running_loss / len(test_loader)\n",
    "\n",
    "    print('Val Loss: {0:.3f}'.format(avg_loss_across_batches))\n",
    "    print('***************************************************')\n",
    "    print()\n",
    "\n",
    "# Set hyperparameters for training\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch()\n",
    "    validate_one_epoch()\n",
    "\n",
    "# Make predictions on the training and testing datasets\n",
    "with torch.no_grad():\n",
    "    predicted = model(X_train.to(device)).to('cpu').numpy()\n",
    "\n",
    "# Visualize the results\n",
    "plt.plot(y_train, label='Actual Tourists')\n",
    "plt.plot(predicted, label='Predicted Tourists')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Tourists')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "train_predictions = predicted.flatten()\n",
    "\n",
    "dummies = np.zeros((X_train.shape[0], lookback+1))\n",
    "dummies[:, 0] = train_predictions\n",
    "dummies = scaler.inverse_transform(dummies)\n",
    "\n",
    "train_predictions = dc(dummies[:, 0])\n",
    "train_predictions\n",
    "\n",
    "# Inverse transform the actual values to the original scale\n",
    "dummies = np.zeros((X_train.shape[0], lookback+1))\n",
    "dummies[:, 0] = y_train.flatten()\n",
    "dummies = scaler.inverse_transform(dummies)\n",
    "\n",
    "new_y_train = dc(dummies[:, 0])\n",
    "new_y_train\n",
    "\n",
    "plt.plot(new_y_train, label='Actual Tourists')\n",
    "plt.plot(train_predictions, label='Predicted Tourists')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Tourists')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "test_predictions = model(X_test.to(device)).detach().cpu().numpy().flatten()\n",
    "\n",
    "dummies = np.zeros((X_test.shape[0], lookback+1))\n",
    "dummies[:, 0] = test_predictions\n",
    "dummies = scaler.inverse_transform(dummies)\n",
    "\n",
    "test_predictions = dc(dummies[:, 0])\n",
    "test_predictions\n",
    "\n",
    "dummies = np.zeros((X_test.shape[0], lookback+1))\n",
    "dummies[:, 0] = y_test.flatten()\n",
    "dummies = scaler.inverse_transform(dummies)\n",
    "\n",
    "new_y_test = dc(dummies[:, 0])\n",
    "new_y_test\n",
    "\n",
    "plt.plot(new_y_test, label='Actual Tourists')\n",
    "plt.plot(test_predictions, label='Predicted Tourists')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Tourists')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Define the metric system for evaluation\n",
    "\n",
    "#Root mean squared error\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "#Mean absolute error\n",
    "def me(predictions, targets):\n",
    "    return (abs(predictions-targets)).mean()\n",
    "\n",
    "#rmse of test predictions\n",
    "rmse(test_predictions,new_y_test)\n",
    "\n",
    "#me of test predictions\n",
    "me(test_predictions,new_y_test)\n",
    "\n",
    "#rmse of train data\n",
    "rmse(new_y_train,train_predictions)\n",
    "\n",
    "#me of train data\n",
    "me(new_y_train,train_predictions)\n",
    "\n",
    "#checking the model parameters\n",
    "model.parameters\n",
    "\n",
    "m = train_predictions.shape[0] + test_predictions.shape[0]\n",
    "\n",
    "predictions = np.zeros(m)\n",
    "\n",
    "for i in range(m):\n",
    "    if(i<train_predictions.shape[0]):\n",
    "        predictions[i] = train_predictions[i]\n",
    "    else:\n",
    "        predictions[i] = test_predictions[i-train_predictions.shape[0]]\n",
    "\n",
    "shifted_df['predictions'] = np.zeros(125)\n",
    "\n",
    "shifted_df['Timestamp'] = shifted_df.index\n",
    "\n",
    "index = np.zeros(125)\n",
    "for i in range(125):\n",
    "    index[i] = i\n",
    "shifted_df['index'] = index\n",
    "shifted_df.set_index('index', inplace = True)\n",
    "\n",
    "for i in range(125):\n",
    "    if i<100:\n",
    "        shifted_df.loc[i,['predictions']] = None\n",
    "    else:\n",
    "        shifted_df.loc[i,['predictions']] = test_predictions[i-100]\n",
    "\n",
    "shifted_df.set_index('Timestamp', inplace = True)\n",
    "\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "loc = plticker.MultipleLocator(base=30.0) # this locator puts ticks at regular intervals\n",
    "\n",
    "#Final plot\n",
    "font = {'family':'serif','color':'black','size':12}\n",
    "fig,ax = plt.subplots()\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "plt.plot(shifted_df['No of Tourist(I'], label='Actual Tourists')\n",
    "plt.plot(shifted_df['predictions'],color='orange', label='Predicted Tourists')\n",
    "plt.xlabel('Time', fontdict = font)\n",
    "plt.ylabel('Tourists', fontdict = font)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "shifted_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc1549",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d4e55",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv(\"D:\\openiitda\\open_new_iit.csv\") #open the file containing the data\n",
    "try:\n",
    "    df['Timestamp']=pd.to_datetime(df['Timestamp'],format='%d-%m-%Y') #convert timestamp to numerical date\n",
    "except ValueError:\n",
    "    print(\"Error\")\n",
    "df['Numerical_date']=df['Timestamp'].dt.month\n",
    "X = df['Numerical_date'] \n",
    "y = df['SUM']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# test train split\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train) #fit the train test data into XGB matrix\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "params = { #set the XGBoost parameters\n",
    "    'objective': 'reg:squarederror',  # for regression\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 500,\n",
    "}\n",
    "model = xgb.train(params, dtrain, num_boost_round=10)  #train the model\n",
    "y_pred = model.predict(dtest) #predict from the trained data\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)  #calculate mean squared error\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "from math import sqrt\n",
    "rmse=sqrt(mse)\n",
    "rmse\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, y_pred)   #calculate mean absolute error\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "xt=df['Numerical_date']\n",
    "yt=df['No of Tourist(I']\n",
    "Xt_train, Xt_test, yt_train, yt_test = train_test_split(X, y, test_size=0.2, random_state=42) #test train split of number of tourists\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "actual_values=yt_test\n",
    "predicted_values=y_pred  #plot the graph\n",
    "\n",
    "time_index=range(len(actual_values))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Actual tourists vs predicted sum of tourists and search volumes\")\n",
    "plt.plot(time_index, actual_values, label='Number of tourists', color='blue', marker='')\n",
    "plt.plot(time_index, predicted_values, label='Sum of trends', color='red', marker='')\n",
    "\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Normalised Volumes\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.315689,
   "end_time": "2023-10-14T18:25:52.611560",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-14T18:25:36.295871",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
